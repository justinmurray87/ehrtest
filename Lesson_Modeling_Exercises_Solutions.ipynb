{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model with TF DenseFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Success - the MySageMakerInstance is in the us-west-2 region. You will use the 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "#this should move to a create lifecycle config\n",
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "\n",
    "#this should move to a create lifecycle config\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/ehrsample2'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "#this should move to a create lifecycle config\n",
    "#create S3 Bucket\n",
    "bucket_name = 'ehrsample2' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should move to a start lifecycle config (or should this be a processing job?)\n",
    "\n",
    "#swiss data set\n",
    "try:\n",
    "  urllib.request.urlretrieve (\"Swiss AWS Path\", \"swiss_set.csv\")\n",
    "  print('Success: downloaded ehr_setn.csv.')\n",
    "except Exception as e:\n",
    "  print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "  swiss_df = pd.read_csv('./swiss_set.csv',index_col=0)\n",
    "  print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "#cleveland data set\n",
    "try:\n",
    "  urllib.request.urlretrieve (\"AWS Path\", \"cleveland_set.csv\")\n",
    "  print('Success: downloaded ehr_setn.csv.')\n",
    "except Exception as e:\n",
    "  print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "  cleveland_df = pd.read_csv('./cleveland_set.csv',index_col=0)\n",
    "  print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "swiss_dataset_path = \"./data/heart_disease_data/processed_swiss.csv\"\n",
    "swiss_df = pd.read_csv(swiss_dataset_path).replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleveland_df = pd.read_csv(\"./data/heart_disease_data/processed.cleveland.txt\",  names=column_list).replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both sources into one\n",
    "combined_heart_df = pd.concat([swiss_df, cleveland_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_heart_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation\n",
    "- This simple verson of the dataset has only a three features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['sex',  'age', 'trestbps', 'thalach' ]\n",
    "bp_df = combined_heart_df[selected_features].replace({1:\"male\", 0:\"female\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST how many rows with at least a single null value\n",
    "sum(bp_df.apply(lambda x: sum(x.isnull().values), axis = 1)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bp_df[bp_df['trestbps'].isnull()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity will drop rows with null since predictor is null \n",
    "clean_df = bp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['trestbps'] = clean_df['trestbps'].astype(float)\n",
    "clean_df['thalach'] = clean_df['thalach'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "def df_to_dataset(df, predictor,  batch_size=32):\n",
    "    df = df.copy()\n",
    "    labels = df.pop(predictor)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 80 20 train test split - not ideal \n",
    "train_dataset = clean_df.sample(frac=0.8,random_state=0)\n",
    "test_dataset = clean_df.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_FIELD = 'trestbps'\n",
    "batch_size = 128\n",
    "train_ds = df_to_dataset(train_dataset, PREDICTOR_FIELD, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_dataset, PREDICTOR_FIELD, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF numeric feature\n",
    "tf_numeric_age_feature = tf.feature_column.numeric_column(key='age', default_value=0, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = [ 0, 18, 25, 40, 55, 65, 80, 100]\n",
    "#create TF bucket feature from numeric feature\n",
    "tf_numeric_age_feature\n",
    "tf_bucket_age_feature = tf.feature_column.bucketized_column(source_column=tf_numeric_age_feature, boundaries= b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using list b/c small number of unique values\n",
    "gender_vocab = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'sex', bp_df['sex'].unique())\n",
    "gender_one_hot = tf.feature_column.indicator_column(gender_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cross features - use example from TF\n",
    "crossed_age_gender_feature = tf.feature_column.crossed_column([tf_bucket_age_feature, gender_vocab], hash_bucket_size=1000)\n",
    "tf_crossed_age_gender_feature = tf.feature_column.indicator_column(crossed_age_gender_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [ tf_crossed_age_gender_feature, tf_bucket_age_feature, gender_one_hot ]\n",
    "dense_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same architecture as example\n",
    "def build_model(dense_feature_layer):\n",
    "  model = tf.keras.Sequential([\n",
    "    dense_feature_layer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(dense_feature_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='mse', patience=50)     \n",
    "history = model.fit(train_ds,   callbacks=[early_stop], epochs=EPOCHS,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(test_ds, verbose=2)\n",
    "print(\"MAE:{}\\nMSE:{}\".format(mae, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_dataset[PREDICTOR_FIELD].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_ds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_outputs = {\n",
    "    \"pred\": test_predictions,\n",
    "    \"actual_value\": test_labels,\n",
    "}\n",
    "model_output_df = pd.DataFrame(model_pred_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Regression Output to binary classification output\n",
    "model_output_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to binary prediction for Brier score - resting bps above 130 \n",
    "def convert_to_binary(df, pred_field, actual_field):\n",
    "    # score is the field name we will use for predictions and is what Aequitas uses\n",
    "    df['score'] = df[pred_field].apply(lambda x: 1 if x>=130 else 0 )\n",
    "    # label_value is the field name we will use for the truth value and is what Aequitas uses\n",
    "    df['label_value'] = df[actual_field].apply(lambda x: 1 if x>=130 else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df = convert_to_binary(model_output_df, 'pred', 'actual_value')\n",
    "binary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = binary_df['label_value'].values \n",
    "y_pred = binary_df['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L4 - 2: Demographic Bias Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "- Using the Compas dataset prepared by Aequitas, perform a Fairness Disparity Analysis on the under 25 Asian female reference group. See the documentation for reference -https://github.com/dssg/aequitas. In particular focus your analysis on fairness and disparity for FPR and where applicable try to leverage some of the visualizations Aequitas provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aequitas Compas Dataset\n",
    "- Using 2016 dataset from ProPublica for automated criminal risk assessment algorithms and adapted from Aequitas notebook - https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb\n",
    "- Preprocessed using this script --https://github.com/dssg/aequitas/blob/master/examples/compas_data_for_aequitas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Aequitas Bias() class is used to calculate disparities between groups based on the crosstab returned by the Group() class get_crosstabs() method we used for preprocessing the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions from Aequitas on Calculating Disparities across Reference Groups\n",
    "(adapted from https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb)\n",
    "\n",
    "Disparities are calculated as a ratio of a metric for a group of interest compared to a base group. For example, the False Negative Rate Disparity for black defendants vis-a-vis whites is:$$Disparity_{FNR} =  \\frac{FNR_{black}}{FNR_{white}}$$\n",
    "\n",
    "Below, we use get_disparity_predefined_groups() which allows us to choose reference groups that clarify the output for the practitioner.\n",
    "\n",
    "The Aequitas Bias() class includes two additional get disparity functions: get_disparity_major_group() and get_disparity_min_metric(), which automate base group selection based on sample majority (across each attribute) and minimum value for each calculated bias metric, respectively.\n",
    "\n",
    "The get_disparity_predefined_groups() allows user to define a base group for each attribute.\n",
    "\n",
    "Disparities Calculated Calcuated:\n",
    "- True Positive Rate Disparity\t'tpr_disprity'\n",
    "- True Negative Rate\t'tnr_disparity'\n",
    "- False Omission Rate\t'for_disparity'\n",
    "- False Discovery Rate\t'fdr_disparity'\n",
    "- False Positive Rate\t'fpr_disparity'\n",
    "- False NegativeRate\t'fnr_disparity'\n",
    "- Negative Predictive Value\t'npv_disparity'\n",
    "- Precision Disparity\t'precision_disparity'\n",
    "- Predicted Positive Ratio$_k$ Disparity\t'ppr_disparity'\n",
    "- Predicted Positive Ratio$_g$ Disparity\t'pprev_disparity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret calculated disparity ratios?**\n",
    "The calculated disparities from the dataframe returned by the Bias() class get_disparity_ methods are in relation to a reference group, which will always have a disparity of 1.0.\n",
    "\n",
    "The differences in False Positive Rates, noted in the discussion of the Group() class above, are clarified using the disparity ratio (fpr_disparity). Black people are falsely identified as being high or medium risks 1.9 times the rate for white people.\n",
    "\n",
    "As seen above, False Discovery Rates have much less disparity (fdr_disparity), or fraction of false postives over predicted positive in a group. As reference groups have disparity = 1 by design in Aequitas, the lower disparity is highlighted by the fdr_disparity value close to 1.0 (0.906) for the race attribute group 'African-American' when disparities are calculated using predefined base group 'Caucasian'. Note that COMPAS is calibrated to balance False Positive Rate and False Discovery Rates across groups.\n",
    "\n",
    "\n",
    "**How does the selected reference group affect disparity calculations?**\n",
    "Disparities calculated in the the Aequitas Bias() class based on the crosstab returned by the Group() class get_crosstabs() method can be derived using several different base gorups. In addition to using user-specified groups illustrated above, Aequitas can automate base group selection based on dataset characterisitcs:\n",
    "\n",
    "Evaluating disparities calculated in relation to a different 'race' reference group\n",
    "Changing even one attribute in the predefined groups will alter calculated disparities. When a different pre-defined group 'Hispanic' is used, we can see that Black people are 2.1 times more likely to be falsely identified as being high or medium risks as Hispanic people are (compared to 1.9 times more likely than white people), and even less likely to be falsely identified as low risk when compared to Hispanic people rather than white people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Aequitas Data that was transformed \n",
    "compas_df = pd.read_csv(\"./data/compas_for_aequitas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aequitas\n",
    "from aequitas.preprocessing import preprocess_input_df\n",
    "from aequitas.group import Group\n",
    "from aequitas.plotting import Plot\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "\n",
    "ae_subset_df = compas_df[['sex', 'age_cat', 'race', 'score', 'label_value']]\n",
    "ae_df, _ = preprocess_input_df(ae_subset_df)\n",
    "g = Group()\n",
    "xtab, _ = g.get_crosstabs(ae_df)\n",
    "absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "clean_xtab = xtab.fillna(-1)\n",
    "aqp = Plot()\n",
    "b = Bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized Metric View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "xtab[[col for col in xtab.columns if col not in absolute_metrics]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = aqp.plot_group_metric_all(xtab, metrics=['tpr', 'fpr', 'ppr', 'pprev', 'fnr'], ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Reference Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = b.get_disparity_predefined_groups(clean_xtab, \n",
    "                    original_df=ae_df, \n",
    "                    ref_groups_dict={'race':'Asian', 'sex':'Female', 'age_cat':'Less than 25'},\n",
    "                    alpha=0.05, \n",
    "                    check_significance=False)\n",
    "\n",
    "f = Fairness()\n",
    "fdf = f.get_group_value_fairness(bdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fpr_disparity = aqp.plot_disparity(bdf, group_metric='fpr_disparity', \n",
    "                                       attribute_name='race')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "African Americans are over 5x more likely to be falsely identified as well as Hispanic and Caucasian have over 2x more likely to be falsely identified than Asian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Value Fairness Determination\n",
    "- Red = False/Not Fair\n",
    "- Green = True/Fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_fairness = aqp.plot_fairness_group(fdf, group_metric='fpr', title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L4 - 3: Uncertainty Estimation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "- Given the Swiss heart disease dataset we have been working with, create an uncertainty estimation model that accounts for Epistemic Uncertainty as well. Provide the mean and standard deviation outputs.\n",
    "- https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adapted from Tensorflow Probability Regression tutorial  https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb    \n",
    "'''\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2*n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "            tfp.distributions.Normal(loc=t[..., :n],\n",
    "                                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "            tfp.distributions.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq_prob_model(feature_layer):\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_layer,\n",
    "        tf.keras.layers.Dense(150, activation='relu'),\n",
    "        tf.keras.layers.Dense(75, activation='relu'),\n",
    "           tfp.layers.DenseVariational(1+1, posterior_mean_field, prior_trainable),        \n",
    "        tfp.layers.DistributionLambda(            \n",
    "            lambda t:tfp.distributions.Normal(loc=t[..., :1],\n",
    "                                            scale=1e-3 + tf.math.softplus(0.1 * t[...,1:])\n",
    "                                             )\n",
    "        ),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_prob_model(train_ds,   feature_layer,  epochs=5, loss_metric='mse'):\n",
    "    model = build_seq_prob_model(feature_layer)\n",
    "    negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "    loss = negloglik\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[loss_metric])\n",
    "    history = model.fit(train_ds, \n",
    "                        epochs=epochs)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_model, history = build_prob_model(train_ds, dense_feature_layer,  epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['sex', 'age', 'thalach']\n",
    "x_tst = dict(test_dataset[feature_list])\n",
    "yhat = prob_model(x_tst)\n",
    "prob_preds = prob_model.predict(test_ds)\n",
    "m = yhat.mean()\n",
    "s = yhat.stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_outputs = {\n",
    "    \"pred\": prob_preds.flatten(),\n",
    "     \"actual_value\": test_dataset['trestbps'].values,\n",
    "    \"pred_mean\": m.numpy().flatten(),\n",
    "    \"pred_std\": s.numpy().flatten()\n",
    "}\n",
    "prob_output_df = pd.DataFrame(prob_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = [prob_model(x_tst) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, yhat in enumerate(yhats):\n",
    "    m = np.squeeze(yhat.mean())\n",
    "    s = np.squeeze(yhat.stddev())\n",
    "    print(\"Mean distributions sampled:{}\".format(m))\n",
    "    print(\"Standard deviation distributions sampled:{}\".format(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
